{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0637438e",
   "metadata": {},
   "source": [
    "# Dinoflagellate and Diatom in a Storm Event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3bca5e",
   "metadata": {},
   "source": [
    "In this notebook, we will first generate section plots which show the biological response in 3 typical regions in SoG, and then the time series of 2 planktons and their ratios in 3 regions during the storm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818426b1",
   "metadata": {},
   "source": [
    "## Section Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02fe02ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:107: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:107: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/tmp/ipykernel_2830240/1217458563.py:107: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  ax.set_xlabel('Longitude ($^{\\circ}$E)', fontsize=11)\n",
      "/tmp/ipykernel_2830240/1217458563.py:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  cbar.set_label(f'{var_type} ($\\mu$mol N L$^{{-1}}$)', fontsize=11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Refined_Diatom_2011_20110708_Section1.svg\n",
      "Saved: Refined_Flagellate_2011_20110708_Section1.svg\n",
      "Saved: Refined_Diatom_2011_20110708_Section2.svg\n",
      "Saved: Refined_Flagellate_2011_20110708_Section2.svg\n",
      "Saved: Refined_Diatom_2011_20110708_Section3.svg\n",
      "Saved: Refined_Flagellate_2011_20110708_Section3.svg\n",
      "Saved: Refined_Diatom_2011_20110717_Section1.svg\n",
      "Saved: Refined_Flagellate_2011_20110717_Section1.svg\n",
      "Saved: Refined_Diatom_2011_20110717_Section2.svg\n",
      "Saved: Refined_Flagellate_2011_20110717_Section2.svg\n",
      "Saved: Refined_Diatom_2011_20110717_Section3.svg\n",
      "Saved: Refined_Flagellate_2011_20110717_Section3.svg\n",
      "Saved: Refined_Diatom_2018_20180708_Section1.svg\n",
      "Saved: Refined_Flagellate_2018_20180708_Section1.svg\n",
      "Saved: Refined_Diatom_2018_20180708_Section2.svg\n",
      "Saved: Refined_Flagellate_2018_20180708_Section2.svg\n",
      "Saved: Refined_Diatom_2018_20180708_Section3.svg\n",
      "Saved: Refined_Flagellate_2018_20180708_Section3.svg\n",
      "Saved: Refined_Diatom_2018_20180717_Section1.svg\n",
      "Saved: Refined_Flagellate_2018_20180717_Section1.svg\n",
      "Saved: Refined_Diatom_2018_20180717_Section2.svg\n",
      "Saved: Refined_Flagellate_2018_20180717_Section2.svg\n",
      "Saved: Refined_Diatom_2018_20180717_Section3.svg\n",
      "Saved: Refined_Flagellate_2018_20180717_Section3.svg\n",
      "Done! Check results in /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Section_Biology/\n"
     ]
    }
   ],
   "source": [
    "# Bio_Section_Plot.py\n",
    "# 目标：绘制硅藻 (Diatoms) 和 鞭毛藻 (Flagellates) 在东西向断面上的深度分布图。\n",
    "# 色标：使用感知均匀且高对比度的 viridis。\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe  # 用于给文字加描边，防止看不清\n",
    "from matplotlib.colors import LogNorm  # 用于对数色标\n",
    "\n",
    "# --- 1. 配置与路径设置 ---\n",
    "BASE_DIR = '/results2/SalishSea/nowcast-green.201905/'\n",
    "FNAME_HEAD = 'SalishSea_1d_' \n",
    "FNAME_TAIL = '_ptrc_T.nc'\n",
    "NEW_INDIR_RESULTS = '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Section_Biology/'\n",
    "\n",
    "if not os.path.exists(NEW_INDIR_RESULTS):\n",
    "    os.makedirs(NEW_INDIR_RESULTS)\n",
    "\n",
    "# 目标点定义\n",
    "TARGETS_DEFINITIONS = {\n",
    "    'Point_1': {'value': (49.0, -123.25), 'label': 'Section#1'},\n",
    "    'Point_2': {'value': (49.3, -124.0),  'label': 'Section#2'},\n",
    "    'Point_3': {'value': (49.9, -124.8),  'label': 'Section#3'},\n",
    "}\n",
    "\n",
    "# 断面宽度配置 (基于索引偏移)\n",
    "SECTION_CONFIG = {\n",
    "    'Point_1': {'left_offset': 35, 'right_offset': 22},\n",
    "    'Point_2': {'left_offset': 30, 'right_offset': 70},\n",
    "    'Point_3': {'left_offset': 45, 'right_offset': 25},\n",
    "}\n",
    "\n",
    "# 绘图设置\n",
    "year_target = [2011, 2018]\n",
    "month_target = 7\n",
    "target_days = [8, 17]\n",
    "DEPTH_LIMIT = 50\n",
    "\n",
    "# --- 2. 辅助函数 ---\n",
    "\n",
    "def find_nearest_indices_2d(lon_2d, lat_2d, target_lon, target_lat):\n",
    "    dist_sq = (lon_2d - target_lon)**2 + (lat_2d - target_lat)**2\n",
    "    return np.unravel_index(dist_sq.argmin(), lon_2d.shape)\n",
    "\n",
    "def load_bio_data_refined(date_obj, y_idx, x_idx, config, lon_2d):\n",
    "    \"\"\"根据偏移量切出精确的断面范围\"\"\"\n",
    "    file_path = os.path.join(BASE_DIR, date_obj.strftime('%d%b%y').lower(), \n",
    "                             f\"{FNAME_HEAD}{date_obj.strftime('%Y%m%d')}_{date_obj.strftime('%Y%m%d')}{FNAME_TAIL}\")\n",
    "    \n",
    "    if not os.path.exists(file_path): return None\n",
    "    \n",
    "    with nc.Dataset(file_path, 'r') as ncfile:\n",
    "        z_var = ncfile.variables['deptht'][:]\n",
    "        z_idx_limit = np.abs(z_var - DEPTH_LIMIT).argmin()\n",
    "        \n",
    "        # 确定 X 轴切片范围\n",
    "        x_start = max(0, x_idx - config['left_offset'])\n",
    "        x_end = min(lon_2d.shape[1], x_idx + config['right_offset'])\n",
    "        x_slice = slice(x_start, x_end)\n",
    "        \n",
    "        diat = ncfile.variables['diatoms'][0, :z_idx_limit+1, y_idx, x_slice]\n",
    "        flag = ncfile.variables['flagellates'][0, :z_idx_limit+1, y_idx, x_slice]\n",
    "        lon_sec = lon_2d[y_idx, x_slice]\n",
    "        \n",
    "        return diat, flag, lon_sec, z_var[:z_idx_limit+1]\n",
    "\n",
    "def plot_refined_section(data, lon, depths, date_str, year, label, var_type):\n",
    "    \"\"\"\n",
    "    修改点：\n",
    "    1. 使用 np.ma.masked_where 处理 0 值。\n",
    "    2. 设置 ax.set_facecolor 为灰色（陆地颜色）。\n",
    "    3. 去掉 set_title，改用 ax.text 在图内添加日期标签。\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # --- 修改1：设置背景色（即陆地/无效值的颜色）---\n",
    "    # 'darkgray' 或者 '#bfbfbf' 都是比较标准的陆地填充色\n",
    "    ax.set_facecolor('darkgray') \n",
    "    \n",
    "    # 针对不同生物设置色标和范围\n",
    "    if var_type == 'Diatom':\n",
    "        cmap = 'YlGnBu' \n",
    "        vmin, vmax = 0.1, 15  # 稍微调整一下范围适应 log\n",
    "    else:\n",
    "        cmap = 'RdPu'   \n",
    "        vmin, vmax = 0.05, 5\n",
    "\n",
    "    # --- 修改2：数据掩膜 (Masking) ---\n",
    "    # 将 <= 0 的数据掩盖掉（变为无效值），这样 pcolormesh 就不会绘制这些点\n",
    "    # 从而露出底下的灰色背景\n",
    "    data_masked = np.ma.masked_less_equal(data, 0)\n",
    "\n",
    "    X, Z = np.meshgrid(lon, depths)\n",
    "    \n",
    "    # 绘图 (注意使用 masked 数据)\n",
    "    cf = ax.pcolormesh(X, Z, data_masked, \n",
    "                       norm=LogNorm(vmin=vmin, vmax=vmax), \n",
    "                       cmap=cmap, shading='auto')\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylim(depths.max(), 0) # 确保Y轴也是从0开始向下\n",
    "    \n",
    "    # 坐标轴标签\n",
    "    ax.set_ylabel('Depth (m)', fontsize=11)\n",
    "    ax.set_xlabel('Longitude ($^{\\circ}$E)', fontsize=11)\n",
    "    \n",
    "    # --- 修改3：移除标题，添加图内文字 ---\n",
    "    # transform=ax.transAxes 表示使用相对坐标 (0-1)\n",
    "    # (0.98, 0.03) 表示右下角; ha='right', va='bottom' 对齐方式\n",
    "    text_content = f\"{label}\\n{date_str}\"\n",
    "    \n",
    "    t = ax.text(0.97, 0.05, text_content, \n",
    "                transform=ax.transAxes, \n",
    "                ha='right', va='bottom', \n",
    "                fontsize=14, fontweight='bold', color='white')\n",
    "    \n",
    "    # 给文字加个黑色描边，防止背景太浅看不清\n",
    "    t.set_path_effects([pe.withStroke(linewidth=2, foreground='black')])\n",
    "\n",
    "    # 色标\n",
    "    cbar = fig.colorbar(cf, ax=ax, extend='both', pad=0.02)\n",
    "    cbar.set_label(f'{var_type} ($\\mu$mol N L$^{{-1}}$)', fontsize=11)\n",
    "    \n",
    "    # 保存\n",
    "    out_name = f\"Refined_{var_type}_{year}_{date_str.replace('-','')}_{label.replace('#','')}.svg\"\n",
    "    plt.savefig(os.path.join(NEW_INDIR_RESULTS, out_name), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {out_name}\")\n",
    "\n",
    "# --- 3. 主程序 ---\n",
    "\n",
    "def main():\n",
    "    # 预加载坐标\n",
    "    with nc.Dataset(os.path.join(BASE_DIR, '08jul11/SalishSea_1d_20110708_20110708_ptrc_T.nc'), 'r') as nf:\n",
    "        lon_2d = nf.variables['nav_lon'][:]\n",
    "        lat_2d = nf.variables['nav_lat'][:]\n",
    "\n",
    "    for year in year_target:\n",
    "        for day in target_days:\n",
    "            curr_date = datetime.date(year, month_target, day)\n",
    "            date_s = curr_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            for p_id, p_def in TARGETS_DEFINITIONS.items():\n",
    "                # 寻找中心点索引\n",
    "                y_idx, x_idx = find_nearest_indices_2d(lon_2d, lat_2d, p_def['value'][1], p_def['value'][0])\n",
    "                \n",
    "                # 加载切片数据\n",
    "                res = load_bio_data_refined(curr_date, y_idx, x_idx, SECTION_CONFIG[p_id], lon_2d)\n",
    "                if res is None: continue\n",
    "                \n",
    "                diat, flag, lon_s, depth_s = res\n",
    "                \n",
    "                # 绘图\n",
    "                plot_refined_section(diat, lon_s, depth_s, date_s, year, p_def['label'], 'Diatom')\n",
    "                plot_refined_section(flag, lon_s, depth_s, date_s, year, p_def['label'], 'Flagellate')\n",
    "\n",
    "    print(f\"Done! Check results in {NEW_INDIR_RESULTS}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f956f",
   "metadata": {},
   "source": [
    "## Region Time Series Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa901ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Processing Target: Point#1 (49.0, -123.25) <<<<\n",
      "  Grid slice: y[393:414], x[277:298], depth[0:19]\n",
      "  -- Year 2011 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting:   0%|          | 0/122 [00:39<?, ?day/s]\n",
      "    Extracting: 100%|██████████| 122/122 [22:59<00:00, 11.31s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Year 2018 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [24:19<00:00, 11.97s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Processing Target: Point#2 (49.3, -124.0) <<<<\n",
      "  Grid slice: y[505:526], x[205:226], depth[0:19]\n",
      "  -- Year 2011 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [19:55<00:00,  9.80s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Year 2018 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [19:46<00:00,  9.73s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Processing Target: Point#3 (49.9, -124.8) <<<<\n",
      "  Grid slice: y[679:700], x[162:183], depth[0:19]\n",
      "  -- Year 2011 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [19:22<00:00,  9.53s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Year 2018 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [19:20<00:00,  9.51s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing results to: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/extracted_biomass_data.pkl\n",
      "Done. Now you can run step2_plot_data.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Extract and Integrate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= 配置与路径 =================\n",
    "# 1. 基础路径 (请确保这些路径在服务器上存在)\n",
    "BASE_DIR = '/results2/SalishSea/nowcast-green.201905/'\n",
    "FILE_AREA = '/results2/SalishSea/nowcast-green.201905/01apr07/SalishSea_1h_20070401_20070401_ptrc_T.nc'\n",
    "FILE_E3T = '/results2/SalishSea/month-avg.201905/SalishSeaCast_1m_carp_T_20070101_20070131.nc'\n",
    "OUTPUT_DIR = '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/'\n",
    "SAVE_FILENAME = 'extracted_biomass_data.pkl'  # 文件名改一下，避免覆盖之前的\n",
    "\n",
    "# 2. 变量名\n",
    "VAR_DIATOMS = 'diatoms'\n",
    "VAR_FLAGELLATES = 'flagellates'\n",
    "DEPTH_LIMIT = 20.0 \n",
    "\n",
    "# 3. 定义目标点\n",
    "TARGETS_DEFINITIONS = {\n",
    "    'Point_1': {'value': (49.0, -123.25), 'label': 'Point#1'},\n",
    "    'Point_2': {'value': (49.3, -124.0),  'label': 'Point#2'},\n",
    "    'Point_3': {'value': (49.9, -124.8),  'label': 'Point#3'},\n",
    "}\n",
    "\n",
    "# 4. 定义区域大小\n",
    "# 半径 10 = 直径 21 个网格点\n",
    "GRID_BOX_RADIUS = 10  \n",
    "\n",
    "# ================= 核心工具函数 =================\n",
    "\n",
    "def get_indices_and_weights(file_area, file_e3t, lat_t, lon_t, radius, depth_limit):\n",
    "    \"\"\"\n",
    "    根据经纬度找到网格索引，并计算该区域前20米的体积权重\n",
    "    \"\"\"\n",
    "    with nc.Dataset(file_area, 'r') as nca, nc.Dataset(file_e3t, 'r') as nce:\n",
    "        lats = nca.variables['nav_lat'][:]\n",
    "        lons = nca.variables['nav_lon'][:]\n",
    "        depths = nce.variables['depth'][:]\n",
    "        \n",
    "        # 1. 深度截断 (找到第一个大于20m的层的索引)\n",
    "        # 注意：SalishSeaCast 模型深度通常固定，这里取第一个大于 limit 的层索引\n",
    "        k_indices = np.where(depths > depth_limit)[0]\n",
    "        k_limit = k_indices[0] if len(k_indices) > 0 else len(depths)\n",
    "        \n",
    "        # 2. 空间搜索 (找最近的点)\n",
    "        dist_sq = (lons - lon_t)**2 + (lats - lat_t)**2\n",
    "        j_c, i_c = np.unravel_index(np.argmin(dist_sq), dist_sq.shape)\n",
    "        \n",
    "        # 3. 确定切片范围 (注意边界处理)\n",
    "        j1 = max(0, j_c - radius)\n",
    "        j2 = j_c + radius + 1\n",
    "        i1 = max(0, i_c - radius)\n",
    "        i2 = i_c + radius + 1\n",
    "        \n",
    "        # 4. 读取该小块的 Area 和 E3T (层厚)\n",
    "        # area shape: (j_sub, i_sub)\n",
    "        area_sub = nca.variables['area'][j1:j2, i1:i2]\n",
    "        # e3t shape: (1, k_limit, j_sub, i_sub) -> 取[0]变成 (k, j, i)\n",
    "        e3t_sub = nce.variables['e3t'][0, :k_limit, j1:j2, i1:i2]\n",
    "        \n",
    "        # 5. 计算体积 (k, j, i)\n",
    "        # 利用广播机制: (j, i) * (k, j, i) -> (k, j, i)\n",
    "        volume_sub = area_sub * e3t_sub\n",
    "        \n",
    "        return j1, j2, i1, i2, k_limit, volume_sub\n",
    "\n",
    "# ================= 主逻辑 =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 确保输出目录存在\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for key, info in TARGETS_DEFINITIONS.items():\n",
    "        print(f\"\\n>>>> Processing Target: {info['label']} {info['value']} <<<<\")\n",
    "        \n",
    "        lat, lon = info['value']\n",
    "        \n",
    "        # 1. 预计算网格和权重 (只做一次)\n",
    "        try:\n",
    "            j1, j2, i1, i2, k_max, vol_weight = get_indices_and_weights(\n",
    "                FILE_AREA, FILE_E3T, lat, lon, GRID_BOX_RADIUS, DEPTH_LIMIT\n",
    "            )\n",
    "            print(f\"  Grid slice: y[{j1}:{j2}], x[{i1}:{i2}], depth[0:{k_max}]\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error locating grid: {e}\")\n",
    "            continue\n",
    "\n",
    "        all_results[key] = {\n",
    "            'meta': info,\n",
    "            'data': {}\n",
    "        }\n",
    "\n",
    "        for year in [2011, 2018]:\n",
    "            print(f\"  -- Year {year} --\")\n",
    "            \n",
    "            # >>>>> 修改点在这里：日期范围改成 6月1日 到 9月30日 <<<<<\n",
    "            start_date = datetime.datetime(year, 6, 1)\n",
    "            end_date = datetime.datetime(year, 9, 30) \n",
    "            \n",
    "            total_days = (end_date - start_date).days + 1\n",
    "            \n",
    "            # 初始化存储列表\n",
    "            times_list = []\n",
    "            d_list = []\n",
    "            f_list = []\n",
    "            ratio_list = []\n",
    "            \n",
    "            pbar = tqdm(total=total_days, desc=f\"    Extracting\", unit=\"day\")\n",
    "            current_date = start_date\n",
    "            \n",
    "            while current_date <= end_date:\n",
    "                # 构造文件路径\n",
    "                date_str = current_date.strftime('%Y%m%d')\n",
    "                folder_date = current_date.strftime('%d%b%y').lower()\n",
    "                fname = f\"SalishSea_1h_{date_str}_{date_str}_ptrc_T.nc\"\n",
    "                file_path = os.path.join(BASE_DIR, folder_date, fname)\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        with nc.Dataset(file_path, 'r') as ds:\n",
    "                            # 提取浓度数据 (time, k, j, i)\n",
    "                            # 假设文件里是每小时数据 (24个时间步)\n",
    "                            conc_d = ds.variables[VAR_DIATOMS][:, :k_max, j1:j2, i1:i2]\n",
    "                            conc_f = ds.variables[VAR_FLAGELLATES][:, :k_max, j1:j2, i1:i2]\n",
    "                            \n",
    "                            # 计算总生物量 (Integral) = sum(Conc * Vol)\n",
    "                            # 结果维度变成 (24,)，即每小时的一个总值\n",
    "                            inv_d_hourly = np.sum(conc_d * vol_weight, axis=(1, 2, 3))\n",
    "                            inv_f_hourly = np.sum(conc_f * vol_weight, axis=(1, 2, 3))\n",
    "                            \n",
    "                            # 降采样：每6小时存一个点\n",
    "                            for h in range(0, 24, 6):\n",
    "                                # 取6小时平均\n",
    "                                val_d = np.mean(inv_d_hourly[h:h+6])\n",
    "                                val_f = np.mean(inv_f_hourly[h:h+6])\n",
    "                                \n",
    "                                # 计算比值 (防除零)\n",
    "                                val_r = val_d / val_f if val_f > 1e-10 else 0.0\n",
    "                                \n",
    "                                # 记录时间 (使用当前日期 + 小时)\n",
    "                                t_point = current_date + datetime.timedelta(hours=h)\n",
    "                                \n",
    "                                times_list.append(t_point)\n",
    "                                d_list.append(val_d)\n",
    "                                f_list.append(val_f)\n",
    "                                ratio_list.append(val_r)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        pbar.write(f\"    Error processing {date_str}: {e}\")\n",
    "                \n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                pbar.update(1)\n",
    "            \n",
    "            pbar.close()\n",
    "            \n",
    "            # 存入字典\n",
    "            all_results[key]['data'][year] = {\n",
    "                'times': times_list,\n",
    "                'diatoms': d_list,\n",
    "                'flagellates': f_list,\n",
    "                'ratio': ratio_list\n",
    "            }\n",
    "\n",
    "    # 保存结果到 PKL\n",
    "    out_pkl = os.path.join(OUTPUT_DIR, SAVE_FILENAME)\n",
    "    print(f\"\\nWriting results to: {out_pkl}\")\n",
    "    with open(out_pkl, 'wb') as f:\n",
    "        pickle.dump(all_results, f)\n",
    "    print(\"Done. Now you can run step2_plot_data.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db717b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Plotting...\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/TimeSeries_3Panel_Point_1.png\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/TimeSeries_3Panel_Point_2.png\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/TimeSeries_3Panel_Point_3.png\n",
      "All plots generated.\n"
     ]
    }
   ],
   "source": [
    "# Plot \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# ================= 配置 =================\n",
    "INPUT_DIR = '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/'\n",
    "DATA_FILENAME = 'extracted_biomass_data.pkl'\n",
    "\n",
    "# 2. 定义关键时间节点\n",
    "BASE_YEAR = 2011\n",
    "STORM_START_MD = (7, 9)   # 风暴开始\n",
    "STORM_END_MD = (7, 17)    # 风暴结束 (持续时间较长)\n",
    "\n",
    "# 3. 绘图聚焦窗口 (X轴范围)\n",
    "# 稍微缩窄一点范围，聚焦在风暴前后\n",
    "PLOT_X_LIM_START = datetime.datetime(BASE_YEAR, 7, 5)\n",
    "PLOT_X_LIM_END = datetime.datetime(BASE_YEAR, 7, 22)\n",
    "\n",
    "# ================= 绘图核心函数 =================\n",
    "def plot_storm_focus_clean(region_key, region_data, save_dir):\n",
    "    label = region_data['meta']['label']\n",
    "    years_data = region_data['data']\n",
    "    \n",
    "    # 创建 3行1列 的画布\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "    \n",
    "    # 颜色定义\n",
    "    color_2011 = 'tab:blue'\n",
    "    color_2018 = 'tab:red'\n",
    "    # 修改：风暴区域颜色 (淡钢蓝，更有风暴的感觉，且不抢眼)\n",
    "    color_storm_area = 'lightsteelblue' \n",
    "    \n",
    "    vars_config = [\n",
    "        ('diatoms', 'Diatoms Inventory (mmol N)', axes[0]),\n",
    "        ('flagellates', 'Flagellates Inventory (mmol N)', axes[1]),\n",
    "        ('ratio', 'Ratio (Diatoms / Flagellates)', axes[2])\n",
    "    ]\n",
    "    \n",
    "    # --- 1. 绘制数据曲线 ---\n",
    "    for year in [2011, 2018]:\n",
    "        if year not in years_data: continue\n",
    "        yd = years_data[year]\n",
    "        \n",
    "        # 对齐时间轴到 2011 年\n",
    "        times = yd['times']\n",
    "        fake_times = [t.replace(year=BASE_YEAR) for t in times]\n",
    "        \n",
    "        # 绘图颜色\n",
    "        c = color_2011 if year == 2011 else color_2018\n",
    "        \n",
    "        for var_name, title, ax in vars_config:\n",
    "            vals = yd[var_name]\n",
    "            # 线条稍微加粗\n",
    "            ax.plot(fake_times, vals, label=f'{year}', color=c, linewidth=2, alpha=0.85)\n",
    "\n",
    "    # --- 2. 添加风暴区域 (遍历所有子图) ---\n",
    "    t_storm_start = datetime.datetime(BASE_YEAR, *STORM_START_MD)\n",
    "    t_storm_end = datetime.datetime(BASE_YEAR, *STORM_END_MD)\n",
    "    \n",
    "    for i, (var_name, title, ax) in enumerate(vars_config):\n",
    "        # A. 绘制风暴区 (无边框)\n",
    "        ax.axvspan(t_storm_start, t_storm_end, color=color_storm_area, alpha=0.4, zorder=0)\n",
    "        \n",
    "        # B. 基础设置\n",
    "        ax.set_ylabel(title, fontsize=10, fontweight='bold')\n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "        \n",
    "        # 图例只在第一张图显示\n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right', frameon=True, framealpha=0.9)\n",
    "\n",
    "        # C. 仅在最上面的图添加 \"Storm Event\" 文字\n",
    "        if i == 0:\n",
    "            y_lims = ax.get_ylim()\n",
    "            text_y = y_lims[1] - (y_lims[1]-y_lims[0])*0.05\n",
    "            # 文字居中于风暴区\n",
    "            mid_storm = t_storm_start + (t_storm_end - t_storm_start)/2\n",
    "            \n",
    "            ax.text(mid_storm, text_y, 'Storm Event\\n(Mixing)', \n",
    "                    ha='center', va='top', color='steelblue', \n",
    "                    fontweight='bold', fontsize=10,\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))\n",
    "\n",
    "    # --- 3. 定制 X 轴 (只操作最下面的图) ---\n",
    "    bottom_ax = axes[2]\n",
    "    \n",
    "    # A. 设置范围\n",
    "    bottom_ax.set_xlim(PLOT_X_LIM_START, PLOT_X_LIM_END)\n",
    "    \n",
    "    # B. 设置特定刻度 (只显示风暴开始和结束)\n",
    "    target_ticks = [t_storm_start, t_storm_end]\n",
    "    bottom_ax.set_xticks(target_ticks)\n",
    "    \n",
    "    # C. 日期格式\n",
    "    bottom_ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "    \n",
    "    # D. 给刻度标签上色\n",
    "    fig.canvas.draw() \n",
    "    xtick_labels = bottom_ax.get_xticklabels()\n",
    "    \n",
    "    for label_obj in xtick_labels:\n",
    "        txt = label_obj.get_text()\n",
    "        # 只要是风暴相关的日期，都标为风暴色\n",
    "        label_obj.set_color('steelblue')\n",
    "        label_obj.set_fontweight('bold')\n",
    "            \n",
    "    # 加重风暴起止的垂直网格线\n",
    "    bottom_ax.grid(True, which='major', axis='x', linestyle='--', linewidth=1.5, color=color_storm_area)\n",
    "\n",
    "    # --- 4. 保存 ---\n",
    "    fig.suptitle(f'Biomass Response to Storm: {label}', fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.1) \n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    out_name = f'StormFocus_Clean_{region_key}.png' # 文件名加了 _Clean\n",
    "    save_path = os.path.join(save_dir, out_name)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {save_path}\")\n",
    "\n",
    "\n",
    "# ================= 主程序 =================\n",
    "if __name__ == \"__main__\":\n",
    "    pkl_path = os.path.join(INPUT_DIR, DATA_FILENAME)\n",
    "    \n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"Error: Data file not found: {pkl_path}\")\n",
    "        exit()\n",
    "        \n",
    "    print(f\"Loading data from {DATA_FILENAME}...\")\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "        \n",
    "    print(\"Generating clean focused plots...\")\n",
    "    for key, data in all_results.items():\n",
    "        plot_storm_focus_clean(key, data, OUTPUT_DIR)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb45503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_biomass_data.pkl...\n",
      "Generating clean focused plots...\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/Storm_Focus_Plots/StormFocus_Clean_Point_1.svg\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/Storm_Focus_Plots/StormFocus_Clean_Point_2.svg\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/Storm_Focus_Plots/StormFocus_Clean_Point_3.svg\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# During Storm Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# ================= 配置 =================\n",
    "# 1. 路径与文件\n",
    "INPUT_DIR = '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/'\n",
    "DATA_FILENAME = 'extracted_biomass_data.pkl'\n",
    "OUTPUT_DIR = os.path.join(INPUT_DIR, 'Storm_Focus_Plots') \n",
    "\n",
    "# 2. 定义关键时间节点\n",
    "BASE_YEAR = 2011\n",
    "STORM_START_MD = (7, 9)   # 风暴开始\n",
    "STORM_END_MD = (7, 17)    # 风暴结束 (持续时间较长)\n",
    "\n",
    "# 3. 绘图聚焦窗口 (X轴范围)\n",
    "# 稍微缩窄一点范围，聚焦在风暴前后\n",
    "PLOT_X_LIM_START = datetime.datetime(BASE_YEAR, 7, 5)\n",
    "PLOT_X_LIM_END = datetime.datetime(BASE_YEAR, 7, 22)\n",
    "\n",
    "# ================= 绘图核心函数 =================\n",
    "def plot_storm_focus_clean(region_key, region_data, save_dir):\n",
    "    label = region_data['meta']['label']\n",
    "    years_data = region_data['data']\n",
    "    \n",
    "    # 创建 3行1列 的画布\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "    \n",
    "    # 颜色定义\n",
    "    color_2011 = 'tab:blue'\n",
    "    color_2018 = 'tab:red'\n",
    "    # 修改：风暴区域颜色 (淡钢蓝，更有风暴的感觉，且不抢眼)\n",
    "    color_storm_area = 'lightsteelblue' \n",
    "    \n",
    "    vars_config = [\n",
    "        ('diatoms', 'Diatoms Inventory (mmol N)', axes[0]),\n",
    "        ('flagellates', 'Flagellates Inventory (mmol N)', axes[1]),\n",
    "        ('ratio', 'Ratio (Diatoms / Flagellates)', axes[2])\n",
    "    ]\n",
    "    \n",
    "    # --- 1. 绘制数据曲线 ---\n",
    "    for year in [2011, 2018]:\n",
    "        if year not in years_data: continue\n",
    "        yd = years_data[year]\n",
    "        \n",
    "        # 对齐时间轴到 2011 年\n",
    "        times = yd['times']\n",
    "        fake_times = [t.replace(year=BASE_YEAR) for t in times]\n",
    "        \n",
    "        # 绘图颜色\n",
    "        c = color_2011 if year == 2011 else color_2018\n",
    "        \n",
    "        for var_name, title, ax in vars_config:\n",
    "            vals = yd[var_name]\n",
    "            # 线条稍微加粗\n",
    "            ax.plot(fake_times, vals, label=f'{year}', color=c, linewidth=2, alpha=0.85)\n",
    "\n",
    "    # --- 2. 添加风暴区域 (遍历所有子图) ---\n",
    "    t_storm_start = datetime.datetime(BASE_YEAR, *STORM_START_MD)\n",
    "    t_storm_end = datetime.datetime(BASE_YEAR, *STORM_END_MD)\n",
    "    \n",
    "    for i, (var_name, title, ax) in enumerate(vars_config):\n",
    "        # A. 绘制风暴区 (无边框)\n",
    "        ax.axvspan(t_storm_start, t_storm_end, color=color_storm_area, alpha=0.4, zorder=0)\n",
    "        \n",
    "        # B. 基础设置\n",
    "        ax.set_ylabel(title, fontsize=10, fontweight='bold')\n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "        \n",
    "        # 图例只在第一张图显示\n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right', frameon=True, framealpha=0.9)\n",
    "\n",
    "        # C. 仅在最上面的图添加 \"Storm Event\" 文字\n",
    "        if i == 0:\n",
    "            y_lims = ax.get_ylim()\n",
    "            text_y = y_lims[1] - (y_lims[1]-y_lims[0])*0.05\n",
    "            # 文字居中于风暴区\n",
    "            mid_storm = t_storm_start + (t_storm_end - t_storm_start)/2\n",
    "            \n",
    "            ax.text(mid_storm, text_y, 'Storm Event', \n",
    "                    ha='center', va='top', color='steelblue', \n",
    "                    fontweight='bold', fontsize=10,\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))\n",
    "\n",
    "    # --- 3. 定制 X 轴 (只操作最下面的图) ---\n",
    "    bottom_ax = axes[2]\n",
    "    \n",
    "    # A. 设置范围\n",
    "    bottom_ax.set_xlim(PLOT_X_LIM_START, PLOT_X_LIM_END)\n",
    "    \n",
    "    # B. 设置特定刻度 (只显示风暴开始和结束)\n",
    "    target_ticks = [t_storm_start, t_storm_end]\n",
    "    bottom_ax.set_xticks(target_ticks)\n",
    "    \n",
    "    # C. 日期格式\n",
    "    bottom_ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "    \n",
    "    # D. 给刻度标签上色\n",
    "    fig.canvas.draw() \n",
    "    xtick_labels = bottom_ax.get_xticklabels()\n",
    "    \n",
    "    for label_obj in xtick_labels:\n",
    "        txt = label_obj.get_text()\n",
    "        # 只要是风暴相关的日期，都标为风暴色\n",
    "        label_obj.set_color('steelblue')\n",
    "        label_obj.set_fontweight('bold')\n",
    "            \n",
    "    # 加重风暴起止的垂直网格线\n",
    "    bottom_ax.grid(True, which='major', axis='x', linestyle='--', linewidth=1.5, color=color_storm_area)\n",
    "\n",
    "    # --- 4. 保存 ---\n",
    "    fig.suptitle(f'Biomass Response to Storm: {label}', fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.1) \n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    out_name = f'StormFocus_Clean_{region_key}.svg' # 文件名加了 _Clean\n",
    "    save_path = os.path.join(save_dir, out_name)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {save_path}\")\n",
    "\n",
    "\n",
    "# ================= 主程序 =================\n",
    "if __name__ == \"__main__\":\n",
    "    pkl_path = os.path.join(INPUT_DIR, DATA_FILENAME)\n",
    "    \n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"Error: Data file not found: {pkl_path}\")\n",
    "        exit()\n",
    "        \n",
    "    print(f\"Loading data from {DATA_FILENAME}...\")\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "        \n",
    "    print(\"Generating clean focused plots...\")\n",
    "    for key, data in all_results.items():\n",
    "        plot_storm_focus_clean(key, data, OUTPUT_DIR)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
