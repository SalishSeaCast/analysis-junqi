{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0637438e",
   "metadata": {},
   "source": [
    "# Dinoflagellate and Diatom in a Storm Event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3bca5e",
   "metadata": {},
   "source": [
    "In this notebook, we will first generate section plots which show the biological response in 3 typical regions in SoG, and then the time series of 2 planktons and their ratios in 3 regions during the storm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818426b1",
   "metadata": {},
   "source": [
    "## Section Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fe02ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:107: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:107: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/tmp/ipykernel_3939911/2089508168.py:107: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  ax.set_xlabel('Longitude ($^{\\circ}$E)', fontsize=11)\n",
      "/tmp/ipykernel_3939911/2089508168.py:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  cbar.set_label(f'{var_type} ($\\mu$mol N L$^{{-1}}$)', fontsize=11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Refined_Diatom_2011_20110708_Section1.svg\n",
      "Saved: Refined_Flagellate_2011_20110708_Section1.svg\n",
      "Saved: Refined_Diatom_2011_20110708_Section2.svg\n",
      "Saved: Refined_Flagellate_2011_20110708_Section2.svg\n",
      "Saved: Refined_Diatom_2011_20110708_Section3.svg\n",
      "Saved: Refined_Flagellate_2011_20110708_Section3.svg\n",
      "Saved: Refined_Diatom_2011_20110720_Section1.svg\n",
      "Saved: Refined_Flagellate_2011_20110720_Section1.svg\n",
      "Saved: Refined_Diatom_2011_20110720_Section2.svg\n",
      "Saved: Refined_Flagellate_2011_20110720_Section2.svg\n",
      "Saved: Refined_Diatom_2011_20110720_Section3.svg\n",
      "Saved: Refined_Flagellate_2011_20110720_Section3.svg\n",
      "Saved: Refined_Diatom_2018_20180708_Section1.svg\n",
      "Saved: Refined_Flagellate_2018_20180708_Section1.svg\n",
      "Saved: Refined_Diatom_2018_20180708_Section2.svg\n",
      "Saved: Refined_Flagellate_2018_20180708_Section2.svg\n",
      "Saved: Refined_Diatom_2018_20180708_Section3.svg\n",
      "Saved: Refined_Flagellate_2018_20180708_Section3.svg\n",
      "Saved: Refined_Diatom_2018_20180720_Section1.svg\n",
      "Saved: Refined_Flagellate_2018_20180720_Section1.svg\n",
      "Saved: Refined_Diatom_2018_20180720_Section2.svg\n",
      "Saved: Refined_Flagellate_2018_20180720_Section2.svg\n",
      "Saved: Refined_Diatom_2018_20180720_Section3.svg\n",
      "Saved: Refined_Flagellate_2018_20180720_Section3.svg\n",
      "Done! Check results in /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Section_Biology/\n"
     ]
    }
   ],
   "source": [
    "# Bio_Section_Plot.py\n",
    "# 目标：绘制硅藻 (Diatoms) 和 鞭毛藻 (Flagellates) 在东西向断面上的深度分布图。\n",
    "# 色标：使用感知均匀且高对比度的 viridis。\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe  # 用于给文字加描边，防止看不清\n",
    "from matplotlib.colors import LogNorm  # 用于对数色标\n",
    "\n",
    "# --- 1. 配置与路径设置 ---\n",
    "BASE_DIR = '/results2/SalishSea/nowcast-green.201905/'\n",
    "FNAME_HEAD = 'SalishSea_1d_' \n",
    "FNAME_TAIL = '_ptrc_T.nc'\n",
    "NEW_INDIR_RESULTS = '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Section_Biology/'\n",
    "\n",
    "if not os.path.exists(NEW_INDIR_RESULTS):\n",
    "    os.makedirs(NEW_INDIR_RESULTS)\n",
    "\n",
    "# 目标点定义\n",
    "TARGETS_DEFINITIONS = {\n",
    "    'Point_1': {'value': (49.0, -123.25), 'label': 'Section#1'},\n",
    "    'Point_2': {'value': (49.3, -124.0),  'label': 'Section#2'},\n",
    "    'Point_3': {'value': (49.9, -124.8),  'label': 'Section#3'},\n",
    "}\n",
    "\n",
    "# 断面宽度配置 (基于索引偏移)\n",
    "SECTION_CONFIG = {\n",
    "    'Point_1': {'left_offset': 35, 'right_offset': 22},\n",
    "    'Point_2': {'left_offset': 30, 'right_offset': 70},\n",
    "    'Point_3': {'left_offset': 45, 'right_offset': 25},\n",
    "}\n",
    "\n",
    "# 绘图设置\n",
    "year_target = [2011, 2018]\n",
    "month_target = 7\n",
    "target_days = [8, 20]\n",
    "DEPTH_LIMIT = 50\n",
    "\n",
    "# --- 2. 辅助函数 ---\n",
    "\n",
    "def find_nearest_indices_2d(lon_2d, lat_2d, target_lon, target_lat):\n",
    "    dist_sq = (lon_2d - target_lon)**2 + (lat_2d - target_lat)**2\n",
    "    return np.unravel_index(dist_sq.argmin(), lon_2d.shape)\n",
    "\n",
    "def load_bio_data_refined(date_obj, y_idx, x_idx, config, lon_2d):\n",
    "    \"\"\"根据偏移量切出精确的断面范围\"\"\"\n",
    "    file_path = os.path.join(BASE_DIR, date_obj.strftime('%d%b%y').lower(), \n",
    "                             f\"{FNAME_HEAD}{date_obj.strftime('%Y%m%d')}_{date_obj.strftime('%Y%m%d')}{FNAME_TAIL}\")\n",
    "    \n",
    "    if not os.path.exists(file_path): return None\n",
    "    \n",
    "    with nc.Dataset(file_path, 'r') as ncfile:\n",
    "        z_var = ncfile.variables['deptht'][:]\n",
    "        z_idx_limit = np.abs(z_var - DEPTH_LIMIT).argmin()\n",
    "        \n",
    "        # 确定 X 轴切片范围\n",
    "        x_start = max(0, x_idx - config['left_offset'])\n",
    "        x_end = min(lon_2d.shape[1], x_idx + config['right_offset'])\n",
    "        x_slice = slice(x_start, x_end)\n",
    "        \n",
    "        diat = ncfile.variables['diatoms'][0, :z_idx_limit+1, y_idx, x_slice]\n",
    "        flag = ncfile.variables['flagellates'][0, :z_idx_limit+1, y_idx, x_slice]\n",
    "        lon_sec = lon_2d[y_idx, x_slice]\n",
    "        \n",
    "        return diat, flag, lon_sec, z_var[:z_idx_limit+1]\n",
    "\n",
    "def plot_refined_section(data, lon, depths, date_str, year, label, var_type):\n",
    "    \"\"\"\n",
    "    修改点：\n",
    "    1. 使用 np.ma.masked_where 处理 0 值。\n",
    "    2. 设置 ax.set_facecolor 为灰色（陆地颜色）。\n",
    "    3. 去掉 set_title，改用 ax.text 在图内添加日期标签。\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # --- 修改1：设置背景色（即陆地/无效值的颜色）---\n",
    "    # 'darkgray' 或者 '#bfbfbf' 都是比较标准的陆地填充色\n",
    "    ax.set_facecolor('darkgray') \n",
    "    \n",
    "    # 针对不同生物设置色标和范围\n",
    "    if var_type == 'Diatom':\n",
    "        cmap = 'YlGnBu' \n",
    "        vmin, vmax = 0.1, 15  # 稍微调整一下范围适应 log\n",
    "    else:\n",
    "        cmap = 'RdPu'   \n",
    "        vmin, vmax = 0.05, 5\n",
    "\n",
    "    # --- 修改2：数据掩膜 (Masking) ---\n",
    "    # 将 <= 0 的数据掩盖掉（变为无效值），这样 pcolormesh 就不会绘制这些点\n",
    "    # 从而露出底下的灰色背景\n",
    "    data_masked = np.ma.masked_less_equal(data, 0)\n",
    "\n",
    "    X, Z = np.meshgrid(lon, depths)\n",
    "    \n",
    "    # 绘图 (注意使用 masked 数据)\n",
    "    cf = ax.pcolormesh(X, Z, data_masked, \n",
    "                       norm=LogNorm(vmin=vmin, vmax=vmax), \n",
    "                       cmap=cmap, shading='auto')\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylim(depths.max(), 0) # 确保Y轴也是从0开始向下\n",
    "    \n",
    "    # 坐标轴标签\n",
    "    ax.set_ylabel('Depth (m)', fontsize=11)\n",
    "    ax.set_xlabel('Longitude ($^{\\circ}$E)', fontsize=11)\n",
    "    \n",
    "    # --- 修改3：移除标题，添加图内文字 ---\n",
    "    # transform=ax.transAxes 表示使用相对坐标 (0-1)\n",
    "    # (0.98, 0.03) 表示右下角; ha='right', va='bottom' 对齐方式\n",
    "    text_content = f\"{label}\\n{date_str}\"\n",
    "    \n",
    "    t = ax.text(0.97, 0.05, text_content, \n",
    "                transform=ax.transAxes, \n",
    "                ha='right', va='bottom', \n",
    "                fontsize=14, fontweight='bold', color='white')\n",
    "    \n",
    "    # 给文字加个黑色描边，防止背景太浅看不清\n",
    "    t.set_path_effects([pe.withStroke(linewidth=2, foreground='black')])\n",
    "\n",
    "    # 色标\n",
    "    cbar = fig.colorbar(cf, ax=ax, extend='both', pad=0.02)\n",
    "    cbar.set_label(f'{var_type} ($\\mu$mol N L$^{{-1}}$)', fontsize=11)\n",
    "    \n",
    "    # 保存\n",
    "    out_name = f\"Refined_{var_type}_{year}_{date_str.replace('-','')}_{label.replace('#','')}.svg\"\n",
    "    plt.savefig(os.path.join(NEW_INDIR_RESULTS, out_name), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {out_name}\")\n",
    "\n",
    "# --- 3. 主程序 ---\n",
    "\n",
    "def main():\n",
    "    # 预加载坐标\n",
    "    with nc.Dataset(os.path.join(BASE_DIR, '08jul11/SalishSea_1d_20110708_20110708_ptrc_T.nc'), 'r') as nf:\n",
    "        lon_2d = nf.variables['nav_lon'][:]\n",
    "        lat_2d = nf.variables['nav_lat'][:]\n",
    "\n",
    "    for year in year_target:\n",
    "        for day in target_days:\n",
    "            curr_date = datetime.date(year, month_target, day)\n",
    "            date_s = curr_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            for p_id, p_def in TARGETS_DEFINITIONS.items():\n",
    "                # 寻找中心点索引\n",
    "                y_idx, x_idx = find_nearest_indices_2d(lon_2d, lat_2d, p_def['value'][1], p_def['value'][0])\n",
    "                \n",
    "                # 加载切片数据\n",
    "                res = load_bio_data_refined(curr_date, y_idx, x_idx, SECTION_CONFIG[p_id], lon_2d)\n",
    "                if res is None: continue\n",
    "                \n",
    "                diat, flag, lon_s, depth_s = res\n",
    "                \n",
    "                # 绘图\n",
    "                plot_refined_section(diat, lon_s, depth_s, date_s, year, p_def['label'], 'Diatom')\n",
    "                plot_refined_section(flag, lon_s, depth_s, date_s, year, p_def['label'], 'Flagellate')\n",
    "\n",
    "    print(f\"Done! Check results in {NEW_INDIR_RESULTS}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f956f",
   "metadata": {},
   "source": [
    "## Region Time Series Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa901ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Processing Target: Point#1 (49.0, -123.25) <<<<\n",
      "  Grid slice: y[393:414], x[277:298], depth[0:19]\n",
      "  -- Year 2011 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting:   0%|          | 0/122 [00:39<?, ?day/s]\n",
      "    Extracting: 100%|██████████| 122/122 [22:59<00:00, 11.31s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Year 2018 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [24:19<00:00, 11.97s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Processing Target: Point#2 (49.3, -124.0) <<<<\n",
      "  Grid slice: y[505:526], x[205:226], depth[0:19]\n",
      "  -- Year 2011 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [19:55<00:00,  9.80s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Year 2018 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [19:46<00:00,  9.73s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Processing Target: Point#3 (49.9, -124.8) <<<<\n",
      "  Grid slice: y[679:700], x[162:183], depth[0:19]\n",
      "  -- Year 2011 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [19:22<00:00,  9.53s/day]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Year 2018 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Extracting: 100%|██████████| 122/122 [19:20<00:00,  9.51s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing results to: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/extracted_biomass_data.pkl\n",
      "Done. Now you can run step2_plot_data.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Extract and Integrate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= 配置与路径 =================\n",
    "# 1. 基础路径 (请确保这些路径在服务器上存在)\n",
    "BASE_DIR = '/results2/SalishSea/nowcast-green.201905/'\n",
    "FILE_AREA = '/results2/SalishSea/nowcast-green.201905/01apr07/SalishSea_1h_20070401_20070401_ptrc_T.nc'\n",
    "FILE_E3T = '/results2/SalishSea/month-avg.201905/SalishSeaCast_1m_carp_T_20070101_20070131.nc'\n",
    "OUTPUT_DIR = '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/'\n",
    "SAVE_FILENAME = 'extracted_biomass_data.pkl'  # 文件名改一下，避免覆盖之前的\n",
    "\n",
    "# 2. 变量名\n",
    "VAR_DIATOMS = 'diatoms'\n",
    "VAR_FLAGELLATES = 'flagellates'\n",
    "DEPTH_LIMIT = 20.0 \n",
    "\n",
    "# 3. 定义目标点\n",
    "TARGETS_DEFINITIONS = {\n",
    "    'Point_1': {'value': (49.0, -123.25), 'label': 'Point#1'},\n",
    "    'Point_2': {'value': (49.3, -124.0),  'label': 'Point#2'},\n",
    "    'Point_3': {'value': (49.9, -124.8),  'label': 'Point#3'},\n",
    "}\n",
    "\n",
    "# 4. 定义区域大小\n",
    "# 半径 10 = 直径 21 个网格点\n",
    "GRID_BOX_RADIUS = 10  \n",
    "\n",
    "# ================= 核心工具函数 =================\n",
    "\n",
    "def get_indices_and_weights(file_area, file_e3t, lat_t, lon_t, radius, depth_limit):\n",
    "    \"\"\"\n",
    "    根据经纬度找到网格索引，并计算该区域前20米的体积权重\n",
    "    \"\"\"\n",
    "    with nc.Dataset(file_area, 'r') as nca, nc.Dataset(file_e3t, 'r') as nce:\n",
    "        lats = nca.variables['nav_lat'][:]\n",
    "        lons = nca.variables['nav_lon'][:]\n",
    "        depths = nce.variables['depth'][:]\n",
    "        \n",
    "        # 1. 深度截断 (找到第一个大于20m的层的索引)\n",
    "        # 注意：SalishSeaCast 模型深度通常固定，这里取第一个大于 limit 的层索引\n",
    "        k_indices = np.where(depths > depth_limit)[0]\n",
    "        k_limit = k_indices[0] if len(k_indices) > 0 else len(depths)\n",
    "        \n",
    "        # 2. 空间搜索 (找最近的点)\n",
    "        dist_sq = (lons - lon_t)**2 + (lats - lat_t)**2\n",
    "        j_c, i_c = np.unravel_index(np.argmin(dist_sq), dist_sq.shape)\n",
    "        \n",
    "        # 3. 确定切片范围 (注意边界处理)\n",
    "        j1 = max(0, j_c - radius)\n",
    "        j2 = j_c + radius + 1\n",
    "        i1 = max(0, i_c - radius)\n",
    "        i2 = i_c + radius + 1\n",
    "        \n",
    "        # 4. 读取该小块的 Area 和 E3T (层厚)\n",
    "        # area shape: (j_sub, i_sub)\n",
    "        area_sub = nca.variables['area'][j1:j2, i1:i2]\n",
    "        # e3t shape: (1, k_limit, j_sub, i_sub) -> 取[0]变成 (k, j, i)\n",
    "        e3t_sub = nce.variables['e3t'][0, :k_limit, j1:j2, i1:i2]\n",
    "        \n",
    "        # 5. 计算体积 (k, j, i)\n",
    "        # 利用广播机制: (j, i) * (k, j, i) -> (k, j, i)\n",
    "        volume_sub = area_sub * e3t_sub\n",
    "        \n",
    "        return j1, j2, i1, i2, k_limit, volume_sub\n",
    "\n",
    "# ================= 主逻辑 =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 确保输出目录存在\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for key, info in TARGETS_DEFINITIONS.items():\n",
    "        print(f\"\\n>>>> Processing Target: {info['label']} {info['value']} <<<<\")\n",
    "        \n",
    "        lat, lon = info['value']\n",
    "        \n",
    "        # 1. 预计算网格和权重 (只做一次)\n",
    "        try:\n",
    "            j1, j2, i1, i2, k_max, vol_weight = get_indices_and_weights(\n",
    "                FILE_AREA, FILE_E3T, lat, lon, GRID_BOX_RADIUS, DEPTH_LIMIT\n",
    "            )\n",
    "            print(f\"  Grid slice: y[{j1}:{j2}], x[{i1}:{i2}], depth[0:{k_max}]\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error locating grid: {e}\")\n",
    "            continue\n",
    "\n",
    "        all_results[key] = {\n",
    "            'meta': info,\n",
    "            'data': {}\n",
    "        }\n",
    "\n",
    "        for year in [2011, 2018]:\n",
    "            print(f\"  -- Year {year} --\")\n",
    "            \n",
    "            # >>>>> 修改点在这里：日期范围改成 6月1日 到 9月30日 <<<<<\n",
    "            start_date = datetime.datetime(year, 6, 1)\n",
    "            end_date = datetime.datetime(year, 9, 30) \n",
    "            \n",
    "            total_days = (end_date - start_date).days + 1\n",
    "            \n",
    "            # 初始化存储列表\n",
    "            times_list = []\n",
    "            d_list = []\n",
    "            f_list = []\n",
    "            ratio_list = []\n",
    "            \n",
    "            pbar = tqdm(total=total_days, desc=f\"    Extracting\", unit=\"day\")\n",
    "            current_date = start_date\n",
    "            \n",
    "            while current_date <= end_date:\n",
    "                # 构造文件路径\n",
    "                date_str = current_date.strftime('%Y%m%d')\n",
    "                folder_date = current_date.strftime('%d%b%y').lower()\n",
    "                fname = f\"SalishSea_1h_{date_str}_{date_str}_ptrc_T.nc\"\n",
    "                file_path = os.path.join(BASE_DIR, folder_date, fname)\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        with nc.Dataset(file_path, 'r') as ds:\n",
    "                            # 提取浓度数据 (time, k, j, i)\n",
    "                            # 假设文件里是每小时数据 (24个时间步)\n",
    "                            conc_d = ds.variables[VAR_DIATOMS][:, :k_max, j1:j2, i1:i2]\n",
    "                            conc_f = ds.variables[VAR_FLAGELLATES][:, :k_max, j1:j2, i1:i2]\n",
    "                            \n",
    "                            # 计算总生物量 (Integral) = sum(Conc * Vol)\n",
    "                            # 结果维度变成 (24,)，即每小时的一个总值\n",
    "                            inv_d_hourly = np.sum(conc_d * vol_weight, axis=(1, 2, 3))\n",
    "                            inv_f_hourly = np.sum(conc_f * vol_weight, axis=(1, 2, 3))\n",
    "                            \n",
    "                            # 降采样：每6小时存一个点\n",
    "                            for h in range(0, 24, 6):\n",
    "                                # 取6小时平均\n",
    "                                val_d = np.mean(inv_d_hourly[h:h+6])\n",
    "                                val_f = np.mean(inv_f_hourly[h:h+6])\n",
    "                                \n",
    "                                # 计算比值 (防除零)\n",
    "                                val_r = val_d / val_f if val_f > 1e-10 else 0.0\n",
    "                                \n",
    "                                # 记录时间 (使用当前日期 + 小时)\n",
    "                                t_point = current_date + datetime.timedelta(hours=h)\n",
    "                                \n",
    "                                times_list.append(t_point)\n",
    "                                d_list.append(val_d)\n",
    "                                f_list.append(val_f)\n",
    "                                ratio_list.append(val_r)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        pbar.write(f\"    Error processing {date_str}: {e}\")\n",
    "                \n",
    "                current_date += datetime.timedelta(days=1)\n",
    "                pbar.update(1)\n",
    "            \n",
    "            pbar.close()\n",
    "            \n",
    "            # 存入字典\n",
    "            all_results[key]['data'][year] = {\n",
    "                'times': times_list,\n",
    "                'diatoms': d_list,\n",
    "                'flagellates': f_list,\n",
    "                'ratio': ratio_list\n",
    "            }\n",
    "\n",
    "    # 保存结果到 PKL\n",
    "    out_pkl = os.path.join(OUTPUT_DIR, SAVE_FILENAME)\n",
    "    print(f\"\\nWriting results to: {out_pkl}\")\n",
    "    with open(out_pkl, 'wb') as f:\n",
    "        pickle.dump(all_results, f)\n",
    "    print(\"Done. Now you can run step2_plot_data.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db717b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Plotting...\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/TimeSeries_3Panel_Point_1.png\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/TimeSeries_3Panel_Point_2.png\n",
      "Saved plot: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/TimeSeries_3Panel_Point_3.png\n",
      "All plots generated.\n"
     ]
    }
   ],
   "source": [
    "# Plot \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# ================= 配置 =================\n",
    "INPUT_DIR = '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/'\n",
    "DATA_FILENAME = 'extracted_biomass_data.pkl'\n",
    "\n",
    "# ================= 绘图函数 =================\n",
    "def plot_region(region_key, region_data, save_dir):\n",
    "    \"\"\"\n",
    "    为单个区域画图：3个子图 (Diatoms, Flagellates, Ratio)\n",
    "    \"\"\"\n",
    "    label = region_data['meta']['label']\n",
    "    years_data = region_data['data']\n",
    "    \n",
    "    # 创建画布：3行1列\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "    \n",
    "    # 定义颜色\n",
    "    colors = {2011: 'tab:blue', 2018: 'tab:red'}\n",
    "    \n",
    "    # 遍历三个子图对应的变量\n",
    "    vars_to_plot = [\n",
    "        ('diatoms', 'Diatoms Inventory (mmol N)', axes[0]),\n",
    "        ('flagellates', 'Flagellates Inventory (mmol N)', axes[1]),\n",
    "        ('ratio', 'Ratio (Diatoms / Flagellates)', axes[2])\n",
    "    ]\n",
    "    \n",
    "    for year in [2011, 2018]:\n",
    "        if year not in years_data: continue\n",
    "        \n",
    "        yd = years_data[year]\n",
    "        # 时间轴对齐技巧：把所有年份的时间强制改为 2011 年，以便在同以x轴显示\n",
    "        times = yd['times']\n",
    "        fake_times = [t.replace(year=2011) for t in times]\n",
    "        \n",
    "        # 循环画 3 个变量\n",
    "        for var_name, title, ax in vars_to_plot:\n",
    "            vals = yd[var_name]\n",
    "            ax.plot(fake_times, vals, label=f'{year}', color=colors[year], \n",
    "                    linewidth=1.5, alpha=0.8)\n",
    "\n",
    "    # 统一设置\n",
    "    for i, (var_name, title, ax) in enumerate(vars_to_plot):\n",
    "        ax.set_ylabel(title, fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.4)\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "        # 给子图加标号 (a), (b), (c)\n",
    "        ax.text(0.01, 0.9, f\"({chr(97+i)})\", transform=ax.transAxes, \n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "    # 设置 X 轴格式 (只在最底下的图设置)\n",
    "    axes[2].set_xlabel('Date (Month-Day)', fontsize=12)\n",
    "    axes[2].xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "    \n",
    "    # 总标题\n",
    "    fig.suptitle(f'Summer Phytoplankton Dynamics - {label}', fontsize=16, y=0.95)\n",
    "    \n",
    "    # 保存\n",
    "    plt.tight_layout()\n",
    "    # 留出标题空间\n",
    "    plt.subplots_adjust(top=0.9) \n",
    "    \n",
    "    out_name = f'TimeSeries_3Panel_{region_key}.png'\n",
    "    save_path = os.path.join(save_dir, out_name)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {save_path}\")\n",
    "\n",
    "# ================= 主程序 =================\n",
    "if __name__ == \"__main__\":\n",
    "    pkl_path = os.path.join(INPUT_DIR, DATA_FILENAME)\n",
    "    \n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"Error: Data file not found at {pkl_path}\")\n",
    "        print(\"Please run step1_extract_data.py first.\")\n",
    "        exit()\n",
    "        \n",
    "    print(\"Loading data...\")\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "        \n",
    "    print(\"Plotting...\")\n",
    "    for key, data in all_results.items():\n",
    "        plot_region(key, data, INPUT_DIR)\n",
    "        \n",
    "    print(\"All plots generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb45503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Data file found: /home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/extracted_biomass_jun_sep.pkl\n",
      "Please run step1_extract_data.py first to generate the data.\n",
      "Loading data from extracted_biomass_jun_sep.pkl...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/extracted_biomass_jun_sep.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 149\u001b[39m\n\u001b[32m    146\u001b[39m     exit()\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_FILENAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    150\u001b[39m     all_results = pickle.load(f)\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating focused plots...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda_envs/analysis-junqi/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/extracted_biomass_jun_sep.pkl'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# During Storm Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# ================= 配置 =================\n",
    "# 1. 路径与文件\n",
    "INPUT_DIR = '/home/jqiu/Programing/Projects/analysis-junqi/Diatom_vs_Flagellate_Report/Results_Bio/'\n",
    "DATA_FILENAME = 'extracted_biomass_jun_sep.pkl'\n",
    "OUTPUT_DIR = os.path.join(INPUT_DIR, 'Storm_Focus_Plots') # 新建一个文件夹存这些图\n",
    "\n",
    "# 2. 定义关键时间节点 (Month, Day)\n",
    "# 我们统一用一个“基准年”(比如2011)来处理绘图的时间轴\n",
    "BASE_YEAR = 2011\n",
    "STORM_START_MD = (7, 9)   # 7月9日\n",
    "STORM_END_MD = (7, 17)    # 7月17日\n",
    "RESPONSE_END_MD = (7, 20) # 7月20日 (3天后)\n",
    "\n",
    "# 3. 定义绘图聚焦的时间窗口 (X轴范围)\n",
    "# 例如：从风暴前4天到响应后5天\n",
    "PLOT_X_LIM_START = datetime.datetime(BASE_YEAR, 7, 5)\n",
    "PLOT_X_LIM_END = datetime.datetime(BASE_YEAR, 7, 25)\n",
    "\n",
    "# ================= 绘图核心函数 =================\n",
    "def plot_storm_focus(region_key, region_data, save_dir):\n",
    "    label = region_data['meta']['label']\n",
    "    years_data = region_data['data']\n",
    "    \n",
    "    # 创建 3行1列 的画布\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # 定义绘图变量和样式\n",
    "    colors_year = {2011: 'tab:blue', 2018: 'tab:red'}\n",
    "    vars_config = [\n",
    "        ('diatoms', 'Diatoms Inventory (mmol N)', axes[0]),\n",
    "        ('flagellates', 'Flagellates Inventory (mmol N)', axes[1]),\n",
    "        ('ratio', 'Ratio (Diatoms / Flagellates)', axes[2])\n",
    "    ]\n",
    "    \n",
    "    # --- 1. 绘制数据曲线 ---\n",
    "    for year in [2011, 2018]:\n",
    "        if year not in years_data: continue\n",
    "        yd = years_data[year]\n",
    "        \n",
    "        # 时间轴对齐：将所有数据的时间强制替换为基准年 (2011)\n",
    "        # 这样 2011 和 2018 的 7月9日 就会重叠在一起\n",
    "        times = yd['times']\n",
    "        fake_times = [t.replace(year=BASE_YEAR) for t in times]\n",
    "        \n",
    "        for var_name, title, ax in vars_config:\n",
    "            vals = yd[var_name]\n",
    "            # 稍微加粗线条，提高透明度以便观察重叠\n",
    "            ax.plot(fake_times, vals, label=f'{year}', color=colors_year[year], \n",
    "                    linewidth=2, alpha=0.7)\n",
    "\n",
    "    # --- 2. 添加风暴/响应区域和标注 (遍历所有子图) ---\n",
    "    # 定义关键日期的 datetime 对象 (用于绘图)\n",
    "    t_storm_start = datetime.datetime(BASE_YEAR, *STORM_START_MD)\n",
    "    t_storm_end = datetime.datetime(BASE_YEAR, *STORM_END_MD)\n",
    "    t_response_end = datetime.datetime(BASE_YEAR, *RESPONSE_END_MD)\n",
    "    \n",
    "    for i, (var_name, title, ax) in enumerate(vars_config):\n",
    "        # A. 绘制灰色风暴区\n",
    "        ax.axvspan(t_storm_start, t_storm_end, color='grey', alpha=0.25, zorder=0)\n",
    "        # B. 绘制绿色响应区\n",
    "        ax.axvspan(t_storm_end, t_response_end, color='tab:green', alpha=0.25, zorder=0)\n",
    "        \n",
    "        # C. 基础设置\n",
    "        ax.set_ylabel(title, fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "        # 只在第一个图显示图例\n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper left', frameon=True, facecolor='white', framealpha=0.9)\n",
    "\n",
    "        # D. 在最上面的图添加区域文字标注\n",
    "        if i == 0:\n",
    "            y_lims = ax.get_ylim()\n",
    "            text_y = y_lims[1] - (y_lims[1]-y_lims[0])*0.1 # 顶部往下10%的位置\n",
    "            # 计算中间时间点用于放置文字\n",
    "            mid_storm = t_storm_start + (t_storm_end - t_storm_start)/2\n",
    "            mid_resp = t_storm_end + (t_response_end - t_storm_end)/2\n",
    "            \n",
    "            ax.text(mid_storm, text_y, 'Storm Event\\n(Wind Mixing)', \n",
    "                    ha='center', va='top', color='dimgrey', fontweight='bold')\n",
    "            ax.text(mid_resp, text_y, 'Expected\\nResponse', \n",
    "                    ha='center', va='top', color='darkgreen', fontweight='bold')\n",
    "\n",
    "    # --- 3. 重点：定制 X 轴刻度和样式 (只操作最下面的图) ---\n",
    "    bottom_ax = axes[2]\n",
    "    \n",
    "    # A. 设置显示范围 (聚焦视图)\n",
    "    bottom_ax.set_xlim(PLOT_X_LIM_START, PLOT_X_LIM_END)\n",
    "    \n",
    "    # B. 设置指定的刻度位置\n",
    "    target_ticks = [t_storm_start, t_storm_end, t_response_end]\n",
    "    bottom_ax.set_xticks(target_ticks)\n",
    "    \n",
    "    # C. 设置日期格式\n",
    "    date_fmt = mdates.DateFormatter('%b %d')\n",
    "    bottom_ax.xaxis.set_major_formatter(date_fmt)\n",
    "    \n",
    "    # D. 【关键】给刻度标签上色\n",
    "    # 必须在 draw 之后或者设置了 formatter 之后才能获取到正确的文本\n",
    "    fig.canvas.draw() \n",
    "    xtick_labels = bottom_ax.get_xticklabels()\n",
    "    \n",
    "    for label_obj in xtick_labels:\n",
    "        txt = label_obj.get_text()\n",
    "        # 根据文本内容判断上什么色\n",
    "        if txt in [t_storm_start.strftime('%b %d'), t_storm_end.strftime('%b %d')]:\n",
    "            label_obj.set_color('dimgrey')\n",
    "            label_obj.set_fontweight('bold')\n",
    "        elif txt == t_response_end.strftime('%b %d'):\n",
    "            label_obj.set_color('darkgreen')\n",
    "            label_obj.set_fontweight('bold')\n",
    "            \n",
    "    # 添加辅助的垂直网格线强调这几个日期\n",
    "    bottom_ax.grid(True, which='major', axis='x', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "    # --- 4. 保存 ---\n",
    "    fig.suptitle(f'Focus on Storm Period: {label}', fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.1) # 调整子图间距\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    out_name = f'StormFocus_{region_key}.png'\n",
    "    save_path = os.path.join(save_dir, out_name)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved focused plot: {save_path}\")\n",
    "\n",
    "\n",
    "# ================= 主程序 =================\n",
    "if __name__ == \"__main__\":\n",
    "    pkl_path = os.path.join(INPUT_DIR, DATA_FILENAME)\n",
    "    \n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"Error: Data file found: {pkl_path}\")\n",
    "        print(\"Please run step1_extract_data.py first to generate the data.\")\n",
    "        exit()\n",
    "        \n",
    "    print(f\"Loading data from {DATA_FILENAME}...\")\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "        \n",
    "    print(\"Generating focused plots...\")\n",
    "    for key, data in all_results.items():\n",
    "        plot_storm_focus(key, data, OUTPUT_DIR)\n",
    "        \n",
    "    print(\"All plots finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
